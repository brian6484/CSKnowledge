{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "id": "wKvZI78dnMIZYt3J5vSPd",
      "type": "rectangle",
      "x": 596.5222997168248,
      "y": 729.0084039651186,
      "width": 980.7999877929688,
      "height": 836.7999572753907,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "af",
      "roundness": {
        "type": 3
      },
      "seed": 782323367,
      "version": 61,
      "versionNonce": 377896329,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1760663635516,
      "link": null,
      "locked": false
    },
    {
      "id": "K2VZyqFo3-jLiEwOykEvT",
      "type": "text",
      "x": 625.3222875097936,
      "y": 750.2083818398745,
      "width": 927.4193115234375,
      "height": 525,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ag",
      "roundness": null,
      "seed": 1355482567,
      "version": 1222,
      "versionNonce": 287808615,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1760664399657,
      "link": null,
      "locked": false,
      "text": "scenario: monitoring and alerting system:\nclarifying questions:\nso we are not using external payment provider but it is an in-house payment service right? yes\n\"payment failures go undetected for 5-10 mins\" i have a doubt that maybe payment \nsynchronous operations also go into kafka queue in their design and immediately return to user\nbut after workers process that event, they realise theres an error (maybe insuff funds that\ncould have been checked before putting to queue) yes worker fails asyncrhonously so \nsystem need to catch worker processing failures in real time\ndo we have to design log collector also? no, assume it is in centralised system\n\nfunctional req:\nin house payment service's monitoring and alerting system\nneed to be distributed across globally 5 regions like US, APAC, etc\ndetect payment processing failure fast \nimprove false positive rate (40% to 5%)\n\ni didnt think of this\nmetrics - collect custom application metrics from 100+ MSA and ingest real time\nalerting - multi condition alerts and severity levels (critical, warning, info)\nnotif - route alerts via channels and escalation policies\n",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "scenario: monitoring and alerting system:\nclarifying questions:\nso we are not using external payment provider but it is an in-house payment service right? yes\n\"payment failures go undetected for 5-10 mins\" i have a doubt that maybe payment \nsynchronous operations also go into kafka queue in their design and immediately return to user\nbut after workers process that event, they realise theres an error (maybe insuff funds that\ncould have been checked before putting to queue) yes worker fails asyncrhonously so \nsystem need to catch worker processing failures in real time\ndo we have to design log collector also? no, assume it is in centralised system\n\nfunctional req:\nin house payment service's monitoring and alerting system\nneed to be distributed across globally 5 regions like US, APAC, etc\ndetect payment processing failure fast \nimprove false positive rate (40% to 5%)\n\ni didnt think of this\nmetrics - collect custom application metrics from 100+ MSA and ingest real time\nalerting - multi condition alerts and severity levels (critical, warning, info)\nnotif - route alerts via channels and escalation policies\n",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "EBxSdlHls6_YcBxwY-AmW",
      "type": "rectangle",
      "x": 1627.1222753027623,
      "y": 737.6082899056704,
      "width": 1150,
      "height": 820.0001525878907,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ai",
      "roundness": {
        "type": 3
      },
      "seed": 1920659111,
      "version": 82,
      "versionNonce": 757673671,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1760664421962,
      "link": null,
      "locked": false
    },
    {
      "id": "5Rh32BPTrxV5oXXkajYyw",
      "type": "text",
      "x": 1653.3889277278927,
      "y": 759.5416598600975,
      "width": 1090.0592041015625,
      "height": 550,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "aj",
      "roundness": null,
      "seed": 1489555177,
      "version": 1120,
      "versionNonce": 609980489,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1760667255895,
      "link": null,
      "locked": false,
      "text": "non-functional:\nCAP theorem: must be AP as we still want to keep monitoring and alerting system even during network\nsplits until isolated nodes sync\nlater \n\ncalc:\n50k transactions/minute produced across entire system\neach transaction is 1.2Kb\n\ntotal data valume (logs):\n50k/60s * 1.2kb = lets say 1 min is 50s, 1.2Mb/s logs produced = 1.2M*100k = 120Gb/day of logs stored/day\n\ntotal metrics volume:\neach instance produces 15 metrics like cpu usage, memory usage, etc \nsize of each metric data point (timestamp + value) = 12 bytes\nmetrics emitted every 10s\n\n15 metrics * 12 bytes * 6 times = 1Kb/min of metric data produced per instance\n1Kb * 1000 instances = 1Mb/min of metric data produced\n\nso u can see log volume/s is much higher than metric volume produced/s! This is v important in design cuz using\nkafka for metrics is overkill but for log its ok. The pull model for collecting logs is also impt",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "non-functional:\nCAP theorem: must be AP as we still want to keep monitoring and alerting system even during network\nsplits until isolated nodes sync\nlater \n\ncalc:\n50k transactions/minute produced across entire system\neach transaction is 1.2Kb\n\ntotal data valume (logs):\n50k/60s * 1.2kb = lets say 1 min is 50s, 1.2Mb/s logs produced = 1.2M*100k = 120Gb/day of logs stored/day\n\ntotal metrics volume:\neach instance produces 15 metrics like cpu usage, memory usage, etc \nsize of each metric data point (timestamp + value) = 12 bytes\nmetrics emitted every 10s\n\n15 metrics * 12 bytes * 6 times = 1Kb/min of metric data produced per instance\n1Kb * 1000 instances = 1Mb/min of metric data produced\n\nso u can see log volume/s is much higher than metric volume produced/s! This is v important in design cuz using\nkafka for metrics is overkill but for log its ok. The pull model for collecting logs is also impt",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "xMfmBfvIpqS7Qp0KL92qL",
      "type": "rectangle",
      "x": 604.4363680296963,
      "y": 1634.7606703494253,
      "width": 1070.57133265904,
      "height": 1880.428567613875,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "ak",
      "roundness": {
        "type": 3
      },
      "seed": 738164711,
      "version": 215,
      "versionNonce": 2116250281,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1760670321048,
      "link": null,
      "locked": false
    },
    {
      "id": "7A5sfv3v5SJOOIJ0Dg_4o",
      "type": "text",
      "x": 630.1506973405224,
      "y": 1656.61784481406,
      "width": 1022.4591674804688,
      "height": 1875,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "al",
      "roundness": null,
      "seed": 431301289,
      "version": 2551,
      "versionNonce": 1652519337,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1760670325399,
      "link": null,
      "locked": false,
      "text": "design:\ni will not be designing payment service so i will assume starting point as when transaction comes\nto payment service\n\nclient transaction -> payment service \n\ni will be checking payment service whether all business logic is decoupled via kafka queue. For \nsyncrhonous operations that might be decoupled via queue that increases false positives (insuff.\nfunds), request should be rejected with 400 status codes and not be processed further. After\nchecking this,\n\nclient transaction -> payment service -> metrics agent pull from payment service (pull-model) and\npush to kafka queue -> kafka queue (async operations like collecting metrics, \nsaving log and transaction data to db, etc) -> central log storage (maybe they are using Cassandra\nfor high write throughput NO they using log + s3, cass is bad)\n\ni chose pull mdoel to reduce responsibility of payment service and increase availability\n\nthen connect grafana to s3 to see a visualisation of logs and set up alerting channels (pagerduty\nand slack) if metrics exceed threshold\n\nidk wats prometheus/thanos/grafana \n\ncorrect design:\nthe important distinction between logs and metrics is that metrics produced is CONSTANT, while\nlogs depend on the traffic and transaction volume. Furthermore, non functional req theres 1.2mb/s \nof logs produced and 1mb/min of metrics produced so log has much higher volume! \n\nso while log needs a buffer like kafka for that high spikes and data volume, metrics dont need. \nThere needs to be distinction between log and metric collection and storage flow.\n\nmetrics:\npayment service exposes metrics endpoint (/metrics) -> Prometheus scrapes metrics (not logs) every 10s\nand stores it in its time-series db on disk (30 days) -> thanos uplods to S3 for long term storage \n(13 months) and provides global query layer across regions using Grafana and query \nlang called PromQL-> grafana dashboard and alert \nmanager (pagerduty/slack)\n\nmetrics is time-series data, while log is structured so for log u can use AWS Athena.\n\ncustom metrics can be done on the application logic side\n\nto reduce false positive, add MULTI-CONDITION alert (maybe latency AND error rate should both\nbe high to be an error), or add ML model\n\nUS-East: Prometheus → Thanos Sidecar → S3 us-east-1\nUS-West: Prometheus → Thanos Sidecar → S3 us-west-2\nEU:      Prometheus → Thanos Sidecar → S3 eu-west-1\nAPAC:    Prometheus → Thanos Sidecar → S3 ap-south-1\nSA:      Prometheus → Thanos Sidecar → S3 sa-east-1\n\n                ↓ (Thanos Query reads from all buckets)\n                \n            Thanos Query (can be in any region)\n                    ↓\n                Grafana\n```\n\n**Benefits:**\n- ✅ No cross-region write costs (data stays in region)\n- ✅ Faster writes (lower latency)\n- ✅ Regional data sovereignty compliance\n- ✅ If one region fails, others unaffected\n\n**How Thanos Query works:**\n- Configured to read from all 5 S3 buckets\n- Queries in parallel across regions\n- Merges results\n- Only cross-region on READ (queries), not WRITE (storage)\n\nlog:\nservice -> fluentd (separate daemon that pulls from service) -> kafka queue -> loki + s3\n\n\n",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "design:\ni will not be designing payment service so i will assume starting point as when transaction comes\nto payment service\n\nclient transaction -> payment service \n\ni will be checking payment service whether all business logic is decoupled via kafka queue. For \nsyncrhonous operations that might be decoupled via queue that increases false positives (insuff.\nfunds), request should be rejected with 400 status codes and not be processed further. After\nchecking this,\n\nclient transaction -> payment service -> metrics agent pull from payment service (pull-model) and\npush to kafka queue -> kafka queue (async operations like collecting metrics, \nsaving log and transaction data to db, etc) -> central log storage (maybe they are using Cassandra\nfor high write throughput NO they using log + s3, cass is bad)\n\ni chose pull mdoel to reduce responsibility of payment service and increase availability\n\nthen connect grafana to s3 to see a visualisation of logs and set up alerting channels (pagerduty\nand slack) if metrics exceed threshold\n\nidk wats prometheus/thanos/grafana \n\ncorrect design:\nthe important distinction between logs and metrics is that metrics produced is CONSTANT, while\nlogs depend on the traffic and transaction volume. Furthermore, non functional req theres 1.2mb/s \nof logs produced and 1mb/min of metrics produced so log has much higher volume! \n\nso while log needs a buffer like kafka for that high spikes and data volume, metrics dont need. \nThere needs to be distinction between log and metric collection and storage flow.\n\nmetrics:\npayment service exposes metrics endpoint (/metrics) -> Prometheus scrapes metrics (not logs) every 10s\nand stores it in its time-series db on disk (30 days) -> thanos uplods to S3 for long term storage \n(13 months) and provides global query layer across regions using Grafana and query \nlang called PromQL-> grafana dashboard and alert \nmanager (pagerduty/slack)\n\nmetrics is time-series data, while log is structured so for log u can use AWS Athena.\n\ncustom metrics can be done on the application logic side\n\nto reduce false positive, add MULTI-CONDITION alert (maybe latency AND error rate should both\nbe high to be an error), or add ML model\n\nUS-East: Prometheus → Thanos Sidecar → S3 us-east-1\nUS-West: Prometheus → Thanos Sidecar → S3 us-west-2\nEU:      Prometheus → Thanos Sidecar → S3 eu-west-1\nAPAC:    Prometheus → Thanos Sidecar → S3 ap-south-1\nSA:      Prometheus → Thanos Sidecar → S3 sa-east-1\n\n                ↓ (Thanos Query reads from all buckets)\n                \n            Thanos Query (can be in any region)\n                    ↓\n                Grafana\n```\n\n**Benefits:**\n- ✅ No cross-region write costs (data stays in region)\n- ✅ Faster writes (lower latency)\n- ✅ Regional data sovereignty compliance\n- ✅ If one region fails, others unaffected\n\n**How Thanos Query works:**\n- Configured to read from all 5 S3 buckets\n- Queries in parallel across regions\n- Merges results\n- Only cross-region on READ (queries), not WRITE (storage)\n\nlog:\nservice -> fluentd (separate daemon that pulls from service) -> kafka queue -> loki + s3\n\n\n",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "Mu6pH06-fJM0TPZRdtykQ",
      "type": "rectangle",
      "x": 1713.7223602433542,
      "y": 1629.2251345370469,
      "width": 1122.0001220703125,
      "height": 713.9999771118164,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "am",
      "roundness": {
        "type": 3
      },
      "seed": 1866388615,
      "version": 100,
      "versionNonce": 1992937449,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1760670334496,
      "link": null,
      "locked": false
    },
    {
      "id": "Vge8T6_K-febI0XDyNBxX",
      "type": "text",
      "x": 1741.7222381730417,
      "y": 1652.225107834166,
      "width": 1017.8992309570312,
      "height": 150,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "an",
      "roundness": null,
      "seed": 1761861991,
      "version": 434,
      "versionNonce": 1549227113,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1760670743739,
      "link": null,
      "locked": false,
      "text": "alerting:\nprometheus scrapes metrics -> prometheus checks metric data against alert rule every 15s -> \nif exceeded, sends fired alert to alert manager -> alert manager received alert from Prom and routes to\ncorrent destination like Pagerduty/slack/email\n\npagerduty: incident management platform that ensures right person gets alerted  ",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "alerting:\nprometheus scrapes metrics -> prometheus checks metric data against alert rule every 15s -> \nif exceeded, sends fired alert to alert manager -> alert manager received alert from Prom and routes to\ncorrent destination like Pagerduty/slack/email\n\npagerduty: incident management platform that ensures right person gets alerted  ",
      "autoResize": true,
      "lineHeight": 1.25
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff",
    "lockedMultiSelections": {}
  },
  "files": {}
}