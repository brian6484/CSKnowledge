For an interview answer, **mention both the page cache and the LRU mechanism** - it shows deeper understanding:

The Linux kernel uses the **page cache** to cache filesystem data in memory and speed up file access. When you read a file, the kernel loads the data into 4KB pages in RAM and keeps them cached, so subsequent reads are served from memory instead of disk, which is much faster. Writes also go to the page cache first as "dirty pages" and are written back to disk asynchronously, allowing the kernel to batch and optimize disk I/O. The page cache is managed using a **two-tier LRU (Least Recently Used) system** with active and inactive lists - newly cached pages start on the inactive list, and if accessed again, get promoted to the active list. This prevents one-time sequential scans from evicting frequently-used data. When the system needs memory, it reclaims pages from the inactive list first, keeping hot data in the active list protected. The page cache works together with the dentry cache (name-to-inode mappings) and inode cache (metadata) to form a multi-layer caching system, and it dynamically uses free RAM, automatically shrinking when applications need memory.
